\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsfonts,graphicx}
%\usepackage{amsmath}
\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{verbatim}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}



\usepackage{macros}


%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\arabic{page}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
      %\vspace{2mm}
%    \hbox to 6.28in { {\bf Statement of Purpose
%        \hfill Fall 2018} }
	%\begin{center}
       \vspace{1mm}
       \hbox to 4.2in { {\Large \hfill Statement of Purpose \hfill} }
%       \vspace{1mm}
%       \hbox to 6.2in { {\it \hfill School of Computer Science  \hfill Subhojyoti Mukherjee  \hfill}}
%       \vspace{1mm}
%       \hbox to 6.2in { {\it \hfill Carnegie Mellon University  \hfill Ph.D Applicant \hfill} }
       \hbox to 4.2in {\hfill 
       \begin{tabular}{cc} 
       Language Technologies Institute  & Ph.D. Applicant \\
       Carnegie Mellon University & Subhojyoti Mukherjee [\href{https://subhojyoti.github.io/pdf/subho_cv.pdf}{\underline{\color{red}CV}}]
       \end{tabular} \hfill}
        %\hbox to 5.2in { {\large \hfill Subhojyoti Mukherjee \hfill } }
       %\end{center}
      %\vspace{2mm}
      }
   }
   \end{center}
   %\markboth{Lecture #1: #2}{Lecture #1: #2}

%Computing and Mathematical Sciences   
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure #1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

%\newcommand\E{\mathbb{E}}
%\usepackage[scaled]{helvet}
%\renewcommand\familydefault{\sfdefault} 
%\usepackage[T1]{fontenc}
%\usepackage{enumitem}


\begin{document}

%\name{\textsc{Subhojyoti Mukherjee}}
%% Note that addresses can be used for other contact information:
%% -phone numbers
%% -email addresses
%% -linked-in profile
%
%%ALL Lab \& Bio-NLP Group\\
%
%\address{College of Information and Computer Sciences \\University of Massachusetts Amherst\\Amherst, MA 01002}
%\address{Phone: +1 669 208 8939\\Email: \texttt{subho@cs.umass.edu, } \\ \texttt{subhojyotimukherjee22@gmail.com} \\ Website: \href{https://subhojyoti.github.io/}{https://subhojyoti.github.io/}}


%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{A - Title}{Lecturer Name}{scribe-name}

I want to pursue Ph.D. in Computer Science from the Language and Technologies Institute and I aspire to become a professor in this field. My research interests span the areas of \textit{Machine learning, Reinforcement learning, Online Learning, Multi-armed bandits, Applied Probability,} and \textit{Online  Optimization}. I completed M.S (Research) in Computer Science on July 2018 from \textbf{Indian Institute of Technology Madras} under the supervision of  \underline{\color{red}\href{https://www.cse.iitm.ac.in/~ravi/}{Dr. Balaraman Ravindran}} and \underline{\color{red}\href{https://rbc-dsai.iitm.ac.in/members/Nandan\%20Sudarsanam/}{Dr. Nandan Sudarsanam}}. I am currently a first semester Ph.D. candidate at \textbf{University of Massachusetts Amherst} from Fall 2018 but I am looking for Ph.D opportunities in Carnegie Mellon University as my recruiting faculty \underline{\color{red}\href{https://www.microsoft.com/en-us/research/people/akshaykr/}{Dr. Akshay Krishnamurthy}} has left for Microsoft Research and there are no other faculty at the current institute whose research interest matches with mine.

\vspace*{-2.2em}
\section{Completed/Ongoing Research}
\vspace*{-1em}

%Reinforcement learning (RL) is a way of learning quite different from Supervised learning. 

I have mainly focused on theoretical research in Reinforcement learning (RL). To briefly describe, an RL agent employs a sequentially adaptive strategy to select actions based on some environmental feedback that maximizes some long-term performance metric. Within the RL framework, Multi-armed Bandits address one of the fundamental challenges of learning called the exploration-exploitation dilemma which tries to answer the following question: "When should a learning algorithm stop exploring alternative actions and focus on the action that has yielded the maximum payoff till now?" This trade-off is a fundamental challenge in many practical and industrial level problems such as item recommendation to new users for which no prior information exist, administration of medical treatments to suffering patients, design of a speech and dialogue system in Natural Language Processing, and intelligent tutoring system. Some of my research works which got published in premier conferences (or are ongoing) which address these issues are mentioned below.


\textbf{1. Variance aware Bandits:} An important problem which arises in mobile channel recommendation and online item recommendation where there is a separation between testing and deployment phases is how to employ these bandits in a pure-exploration setting. In the pure-exploration \href{https://www.ijcai.org/proceedings/2017/0350.pdf}{\underline{\color{blue}Thresholding Bandits}} \citep{mukherjee2016} we proposed an algorithm which finds the best set of actions with qualities above a particular threshold. The challenge lies in the observation that this best set can be very large and moreover that the candidate set of actions can be exponentially large. Our novel algorithm solves this problem by estimating the mean and the \textit{variance} (hence being variance-aware) of the actions to determine their qualities, thereby significantly reducing the exploration. We also provide theoretical bounds for the maximum possible loss achieved by our
method. This work was published in \textbf{International Joint Conference on Artificial Intelligence 2017}.

%further being

My next work focused on a more fundamental problem regarding exploration-exploitation dilemma in which there has been a theoretical gap in the lower and upper bounds on the loss of a class of strategies called \textit{variance aware elimination} algorithms. These algorithms eliminate actions which are deemed to be sub-par depending on their qualities based on estimated variances. Our proposed algorithm called \href{https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16111}{\underline{\color{blue}Efficient-UCB-Variance}} \citep{mukherjee2018} falls under this class, and we established that the maximum possible loss (regret upper bound) of this strategy matches the lower bound for all classes of strategies in all the possible environments satisfying certain properties. This work has been accepted in \textbf{Association for Advancement of Artificial Intelligence 2018}.

\textbf{2. Changepoint detection and piecewise i.i.d Bandits:}  Applying Bandit strategies in environments where the quality of all the actions change abruptly and simultaneously has proved to be quite challenging. This is quite common in \textit{medical domain} where the behavior of drug-resistant bacteria may change abruptly prompting a different response to all treatments (actions). We proposed two novel algorithms called \href{https://subhojyoti.github.io/pdf/aistats_2019.pdf}{\underline{\color{blue}UCB-CPD}} and \href{https://subhojyoti.github.io/pdf/aistats_2019.pdf}{\underline{\color{blue}ImpCPD}} for this setting with interesting theoretical guarantees and closed some gaps in the existing bounds. This work was jointly done with \href{https://scholar.google.com/citations?user=7EweMdoAAAAJ&hl=en}{\underline{\color{red}Dr. Odalric Maillard}} while I was an intern at INRIA, SequeL Lab, Lille, France, and is currently under review in \textbf{International Conference on Artificial Intelligence and Statistics 2019}. 

%This is a very challenging and interesting domain, generally referred to as the piecewise i.id setting which acts as a bridge between stationary and fully adversarial setting.

\textbf{3. Ranking and Latent Bandits:} Online ranking of items personalized for each user based on their historical preference is an active area of research. By leveraging its low rank structure previous works have mainly focused on its approximate reconstruction using stochastic observations that require many assumptions and costly matrix operations. We mainly focused on intelligently exploring a partially observed user-item matrix without reconstructing it by leveraging the prior knowledge of the rank of the matrix. This is an ongoing joint \href{https://subhojyoti.github.io/pdf/paper.pdf}{\underline{\color{blue} work}} with \href{http://www.bkveton.com/}{\underline{\color{red}Dr. Branislav Kveton}}, and \href{https://sites.google.com/site/anupraob/}{\underline{\color{red}Dr.Anup Rao}}  which was started while I was an intern at Adobe Research, San Jose, and we are planning to publish it soon.

These works and more fundamental insights into several aspects of Bandits, RL and Learning theory have been captured in my \href{https://www.cse.iitm.ac.in/~ravi/papers/Subhojyoti_thesis.pdf}{\underline{\color{blue}MS thesis}} which was accepted by my technical committee at IIT Madras without any major revision.
	 
	 %I further worked on two interesting variations of Bandits with my internship advisers at INRIA, SequeL Lab, Lille France and at Adobe Research Lab, San Jose where I interned in 2017 and 2018 respectively.
	
\vspace*{-2em}
\section{Future Directions (LTI)}
\vspace*{-1em}

Working on theoretical research in RL setting allows me to expand my research frontier in a variety of new but related research areas including but not limited to statistical learning theory, online optimization, and online personalized recommendation and ranking of items. Furthermore, there are areas in Natural Language Processing which will benefit from my knowledge of Reinforcement Learning. A few of the interesting research problems that I intend to work are,

\textbf{1. Intelligent Tutoring Systems:} The task of designing an intelligent question-answer based tutoring system that adapts to the changing nature of the student/viewer is a challenging task. One of the main challenges is that if the system continuously provides either a very trivial task or a very challenging task (questions), then the student might lose interest and leave the task. So, the system must give a sequence of tasks that closely adapts to the learning curve of the student, varying the difficulty level slowly, where the feedback is also in form of text from the student. This property can be captured by, for instance, a recurrent neural network which learns its features based on sequential data, which in this case is represented by the series of tasks given to the student either in words or as images. The adaptive learning behavior of this problem falls under the exploration-exploitation dilemma of RL to which I can contribute.
%Note, the sequential nature of the tutoring system and the exploration-exploitation dilemma it faces, and to this I can contribute heavily.

\textbf{2. Prediction from Patient EHR data:} This is an area that is not only challenging but directly sits across two of my most passionate areas of research, Safe Reinforcement Learning and learning text representations. Note that a typical EHR patient's health record consists of a series of patient-physician interactions resulting in multiple electronic prescriptions (text) for the patient and their feedback. The first challenge which is slightly easier to solve is to find the correlation of diseases and administered/suggested drugs in one such prescription. A more challenging task is to connect many such prescriptions generated sequentially over time and provide a long term prediction with high guarantee. Again, I can draw from my experience in RL and learning theory to contribute to this problem. Currently, one of my research projects focuses on predictions from the MIMIC 3 dataset containing such EHR of patients.


\textbf{3. Text-based Game playing:} In text-based games, the main challenge lies in the ability to learn optimal policies in systems where the action space is defined by sentences in natural language, continuous state space and games with multiple endings and rewards. These feedbacks allow us to employ reinforcement learning techniques and jointly learn text representations and control policies. Again, the main challenge lies in constructing text representations that captures the long term correlations of actions and employing RL to modify the learned policy.
%\textbf{4. Approximation techniques for High Dimensional Data:} Have to write


%\textbf{5. Outlier detection and Diversity in Recommendation:} Have to write

%\textbf{4. Optimal Design and Active Learning:} Many supervised learning tasks operate only on limited labeled data, and hence, the learning models result in high uncertainty. Moreover in certain tasks, it is costly to run experiments, and hence, the learner needs to carefully pick subsets of data to minimize model uncertainty. This can be handled
%in several ways including active learning, online learning, and bandit feedback. This is a combinatorial optimization problem having many open-ended questions and requiring strong theoretical guarantees, to which I can contribute from my past experience.
%needed  come by on several open-ended problems.


\vspace*{-2em}
\section{Program and Faculty of Interest}
\vspace*{-1em}

I believe that a doctoral degree from Carnegie Mellon University will propel me a long way in my goal to become a professor in Computer Science. Its broad reach and in-depth courses on a  variety of subjects ranging from practical to theoretical applications in Computer Science will help me in my endeavor. There are several professors at Carnegie Mellon University such as \textit{Dr. Pradeep Ravikumar} and \textit{Dr. Aaditya Ramdas} whose works/projects are especially appealing to me. Dr. Pradeep Ravikumar works in theoretical machine learning and its applications to high dimensional real-life data-sets which is an area I am interested to work in. Specifically, I want to work with him in the theoretical areas of learning theory and find common solutions where my expertise can be applied. Also, I am interested to work with Dr. Aaditya Ramdas in the area of online learning and optimization. I want to collaborate with him in several of his areas of interest such as automated discovery and efficient data processing in applied sciences including health-sciences and robotics, where there are several bandit variants which can be applied.


%\newpage
\bibliographystyle{aaai}
\bibliography{refs}

%mukherjee2016
%mukherjee2018

\end{document}

