\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsfonts,graphicx}
%\usepackage{amsmath}
\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{verbatim}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}



\usepackage{macros}


%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\arabic{page}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
      %\vspace{2mm}
%    \hbox to 6.28in { {\bf Statement of Purpose
%        \hfill Fall 2018} }
	%\begin{center}
       \vspace{1mm}
       \hbox to 4.2in { {\Large \hfill Statement of Purpose \hfill} }
%       \vspace{1mm}
%       \hbox to 6.2in { {\it \hfill School of Computer Science  \hfill Subhojyoti Mukherjee  \hfill}}
%       \vspace{1mm}
%       \hbox to 6.2in { {\it \hfill Carnegie Mellon University  \hfill Ph.D Applicant \hfill} }
       \hbox to 4.2in {\hfill 
       \begin{tabular}{cc} 
       School of Computer Science  & Ph.D Applicant \\
       Carnegie Mellon University & Subhojyoti Mukherjee
       \end{tabular} \hfill}
        %\hbox to 5.2in { {\large \hfill Subhojyoti Mukherjee \hfill } }
       %\end{center}
      %\vspace{2mm}
      }
   }
   \end{center}
   %\markboth{Lecture #1: #2}{Lecture #1: #2}

%Computing and Mathematical Sciences   
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure #1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}[theorem]{Definition}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

%\newcommand\E{\mathbb{E}}
%\usepackage[scaled]{helvet}
%\renewcommand\familydefault{\sfdefault} 
%\usepackage[T1]{fontenc}
%\usepackage{enumitem}


\begin{document}

%\name{\textsc{Subhojyoti Mukherjee}}
%% Note that addresses can be used for other contact information:
%% -phone numbers
%% -email addresses
%% -linked-in profile
%
%%ALL Lab \& Bio-NLP Group\\
%
%\address{College of Information and Computer Sciences \\University of Massachusetts Amherst\\Amherst, MA 01002}
%\address{Phone: +1 669 208 8939\\Email: \texttt{subho@cs.umass.edu, } \\ \texttt{subhojyotimukherjee22@gmail.com} \\ Website: \href{https://subhojyoti.github.io/}{https://subhojyoti.github.io/}}


%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{A - Title}{Lecturer Name}{scribe-name}

I want to pursue Ph.D. in Computer Science and I aspire to become a professor in this field. My research interests span the areas of \textit{Reinforcement Learning, Online Optimization, and Recommender systems}. I completed M.S (Research) in Computer Science on July 2018 from \textbf{Indian Institute of Technology Madras} where my advisers where \underline{\color{blue}\href{https://www.cse.iitm.ac.in/~ravi/}{Dr. Balaraman Ravindran}} and \underline{\color{blue}\href{https://rbc-dsai.iitm.ac.in/members/Nandan\%20Sudarsanam/}{Dr. Nandan Sudarsanam}}. I am currently a first semester Ph.D. candidate at \textbf{University of Massachusetts Amherst} from Fall 2018 but I am looking for Ph.D opportunities in Carnegie Mellon University as my recruiting faculty \underline{\color{blue}\href{https://www.microsoft.com/en-us/research/people/akshaykr/}{Dr. Akshay Krishnamurthy}} has left for Microsoft Research and there are no other faculty at the current institute whose research interest matches with mine.

%In my pursuit of these areas, I have explicitly focused in the area of Multi-armed Bandit (MAB) in my past works. In recent years, MABs have been increasingly gaining attention in a number of inter-disciplinary areas such as online education, online medical/health recommendation, online advertising, worker productivity management, and several other interesting industrial applications. 

%During my M.S at Indian Institute of Technology Madras, I collaborated with my advisors, \textit{Dr. Balaraman Ravindran} and \textit{Dr. Nandan Sudarsanam} as well as \textit{Dr. K.P. Naveen} from Indian Institute of Technology Tirupati. A few of my past research works that resulted in publications in premier conferences from these collaborations (see \citet{mukherjee2018}, \citet{mukherjee2016}) and a short description of them are listed below:- 

\vspace*{-2.2em}
\section{Completed/Ongoing Research}
\vspace*{-1em}

%Reinforcement learning (RL) is a way of learning quite different from Supervised learning. 

My previous research have mainly focused on the theory aspect of Reinforcement learning (RL). An RL agent employs a sequentially adaptive strategy to select actions based on some environmental feedback that maximizes some long-term performance metric. Within the RL framework, Multi-armed Bandits try to explain one of the fundamental challenges of learning in general called the exploration-exploitation dilemma. The exploration-exploitation trade-off tries to answer the question when should a learning algorithm stop exploring alternative actions and focus on the action that has yielded the maximum payoff till now. This trade-off is a fundamental challenge in many practical and industrial level problems such as recommending items to new users for which no prior information exist; in medical domain trying to administer treatments to patients who are suffering from a disease; in speech and dialogue system in Natural Language Processing, etc. Some of my research works which got published in premiere conferences (or is ongoing) which addresses these issues are  mentioned below.

%Specifically, I developed algorithms for Multi-armed Bandits which try to explain one of the fundamental challenges of learning in general. Bandits try to answer the exploration-exploitation dilemma which is present in many learning paradigms. The exploration-exploitation trade-off tries to answer the question when should a learning algorithm stop exploring alternative actions and focus on the action that has yielded the maximum payoff till now. This trade-off is a fundamental challenge in many practical and industrial level problems such as recommending items to new users for which no prior information exist; in medical domain trying to administer treatments to patients who are suffering from a disease; in speech and dialogue system in Natural Language Processing, etc.

%Some of my finished research works which got published in premiere conferences have addressed these issues in a limited way. 

\textbf{1. Variance aware Bandits:} While variance aware bandits have been studied for quite some time now, there are quite a few issues that still needs attention. One of them is to how to use them in pure-exploration setting and how to close some theoretical gaps. This type of pure exploration problems arises in mobile channel recommendation, online item recommendation where there is a separation between the testing and deploying phase. In the pure-exploration \href{https://www.ijcai.org/proceedings/2017/0350.pdf}{\underline{\color{blue}Thresholding Bandits}} \citep{mukherjee2016} we propose an algorithm which finds the best set of actions all of whose quality is above a particular threshold. This is a complex problem to solve as this best set can be quite large and the candidate set of actions from which to select the best set can be exponentially large. We propose a novel algorithm to solve this problem and give theoretical bounds on the maximum possible loss suffered by our method. My next work focused on a more fundamental problem regarding exploration-exploitation dilemma. For quite some time there has been a theoretical gap in the lower and upper bound on the loss of a class of strategies called variance aware elimination algorithms. These algorithms uses variance estimates to determine the quality of an action and successively eliminates actions which are deemed to be sub-par based on their estimated qualities. Our proposed algorithm called \href{https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16111}{\underline{\color{blue}Efficient-UCB-Variance}} \citep{mukherjee2018} is such a strategy and we established that the maximum possible loss (regret upper bound) of this strategy matches the lower bound for all class of strategies in all the possible environments satisfying certain properties. These works have been accepted in \textbf{IJCAI 2017} and \textbf{AAAI 2018} respectively.

\textbf{2. Changepoint detection and Piecewise i.i.d Bandits:}  Applying Bandit strategies in environments where the quality of all the actions change abruptly and simultaneously has proved to be quite challenging. This is quite common in \textit{medical domain} where the behavior of a drug-resistant bacteria may change abruptly prompting a different response to all the treatments (actions). We proposed two novel algorithms called \href{https://subhojyoti.github.io/pdf/aistats_2019.pdf}{\underline{\color{blue}UCB-CPD}} and \href{https://subhojyoti.github.io/pdf/aistats_2019.pdf}{\underline{\color{blue}ImpCPD}} for this setting with interesting theoretical guarantees and closed some gaps in the existing bounds. This work was jointly done with Dr. Odalric Maillard while I was an intern at INRIA, SequeL Lab,Lille and is currently under review in \textbf{AISTATS 2019}. 

%This is a very challenging and interesting domain, generally referred to as the piecewise i.id setting which acts as a bridge between stationary and fully adversarial setting.

\textbf{3. Ranking and Latent Bandits:} Online ranking of items personalized for each user based on their historical preference is an active area of research. This problem is challenging because the user-item preference matrix can be very large while it has a low rank structure which can be leveraged. Previous works have mainly focused to approximately reconstruct this user-item matrix with stochastic observations which requires a lot of assumptions and costly matrix operations. We mainly focused on intelligently exploring a partially observed user-item matrix without reconstructing it by leveraging the prior knowledge of the rank of the matrix. This is an ongoing \href{https://subhojyoti.github.io/pdf/paper.pdf}{\underline{\color{blue}work}} with Dr. Branislav Kveton, Dr.Anup Rao  while I was an intern at Adobe Research, San Jose which we are planning to publish soon.

These works and more fundamental insights into several aspects of Bandits, RL and Learning theory have been captured in my \href{https://www.cse.iitm.ac.in/~ravi/papers/Subhojyoti_thesis.pdf}{\underline{\color{blue}MS thesis}} which was accepted by my technical committee at IIT Madras without any major revision.
	 
	 %I further worked on two interesting variations of Bandits with my internship advisers at INRIA, SequeL Lab, Lille France and at Adobe Research Lab, San Jose where I interned in 2017 and 2018 respectively.
	
\vspace*{-2em}
\section{Future Directions}
\vspace*{-1em}

%Although I have mainly focused on immediate Reinforcement learning settings and mostly theoretical works, I am open to a variety of research areas  to bridge the gap between statistical learning theory and their empirical performance. A few of the interesting research problems that I intend to work are,

Working on theoretical research in RL setting allows me to expand my research frontier in a variety of new but related research areas including but not limited to statistical learning theory, optimization. I am open to a variety of research areas  to bridge the gap between statistical learning theory and their empirical performance. A few of the interesting research problems that I intend to work are,

\textbf{1. Off Policy Reinforcement Learning:} Often the complete knowledge of the world is not available to the learner and this forces it to adopt different strategies. Some of the motivating examples of these are, a robot trying to navigate a maze using faulty sensors, or administering medical treatments to patients with only a partial knowledge of the true state of the patient (as it is requires information to an atomic level) or even in recommender systems trying to figure out user preferences from past logs without actually subjecting user to cumbersome exploration strategies. In RL,  partial observability is a broad area of research and theoretical guarantees are hard to give because of the high variance in the performance of various strategies due to limited exposure to the state space. This is related to my previous experience as this again boils down to conducting intelligent exploration to reduce variance in  performance as well as give a theoretical guarantee on how worse the performance can be.


\textbf{2. Safe Reinforcement Learning:} I am interested in safe RL and it has some of the same challenges posed above. For example, in \textit{medical domain} not only we have to work in a partial observable environment but also give confidence guarantees for the proposed action (medical treatment).This gives confidence to the medical practitioner in a high-risk environment where faulty actions might result in death. This is again directly related to my previous experience as this requires a sound knowledge of concentration inequalities to give the confidence on an action taken. In several areas of robotics safe RL is of extreme importance, for example in autonomous driving, motion planning and control, etc.


%\textbf{3. Interactive Active Learning:}  Have to read some papers.


\textbf{3. High Dimensional Reinforcement Learning:} There has been several recent successes of RL for large state spaces like the game Go or in autonomous driving. In large state spaces the success of RL has come by employing powerful non-linear function approximation techniques like Neural Networks. But sound theoretical guarantees for RL only exist for small state spaces using linear function approximation techniques. I am interested in bridging this gap between the theoretical and empirical performance of RL as this will directly benefit some of the challenges mentioned before and I can draw my knowledge from concentration measure theory and RL to contribute to this.

%\textbf{4. Approximation techniques for High Dimensional Data:} Have to write


%\textbf{5. Outlier detection and Diversity in Recommendation:} Have to write

\textbf{4. Optimal Design and Active Learning:} In many supervised learning tasks experiments needs to be run on limited labeled data, and the learning  models have uncertainty in their prediction. Moreover in certain tasks it's costly to run experiments and so the learner needs to carefully pick and choose subsets of the data to minimize the model uncertainty. This can be handled in several ways, like active learning, online learning, using bandits, etc.  This is a combinatorial optimization problem and theoretical guarantees are had to come by on several open-ended problems.


\vspace*{-2em}
\section{Program and Faculty of Interest}
\vspace*{-1em}

I believe that a doctoral degree from Carnegie Mellon University will propel me a long way in my goal to become a professor in Computer Science. Its broad reach and in-depth courses on a  variety of subjects ranging from practical to theoretical applications in Computer Science will help me in my endeavor. There are several professors at Carnegie Mellon University such as \textit{Dr. Pradeep Ravikumar} and \textit{Dr. Aaditya Ramdas} whose works/projects are especially appealing to me. Dr. Pradeep Ravikumar works in theoretical machine learning and its applications to high dimensional real-life data-sets which is an area I am interested to work in. Specifically, I want to work with him in the theoretical areas of learning theory and find common solutions where my expertise can be applied. Also, I am interested to work with Dr. Aaditya Ramdas in the area of online learning and optimization. I want to collaborate with him in several of his areas of interest such as automated discovery and efficient data processing in applied sciences including health-sciences and robotics, where there are several bandit variants which can be applied.

%\begin{enumerate}
%\item \textbf{Efficient-UCBV: An Almost Optimal Algorithm using Variance Estimates:} In this work, we focus on the simple stochastic bandit model. We propose a novel variant of the UCB algorithm (referred to as Efficient-UCB-Variance (EUCBV)) for minimizing cumulative regret in the stochastic multi-armed bandit (MAB) setting. EUCBV incorporates the arm elimination strategy proposed in UCB-Improved, while taking into account the variance estimates to compute the arms' confidence bounds, similar to UCBV. Through a theoretical analysis we establish that EUCBV incurs a \emph{gap-dependent} regret bound of  $O\left( \dfrac{K\sigma^2_{\max} \log (T\Delta^2 /K)}{\Delta}\right)$ after $T$ trials, where $\Delta$ is the minimal gap between optimal and sub-optimal arms; the above bound is an improvement over that of existing state-of-the-art UCB algorithms (such as UCB1, UCB-Improved, UCBV,  MOSS). Further, EUCBV incurs a \emph{gap-independent} regret bound of {$O\left(\sqrt{KT}\right)$}  which is an improvement over that of UCB1, UCBV and UCB-Improved, while being comparable with that of MOSS and OCUCB. Through an extensive numerical study, we show that EUCBV significantly outperforms the popular UCB variants (like MOSS, OCUCB, etc.) as well as Thompson sampling and Bayes-UCB algorithms. This work has been accepted for publication in \textit{Proceedings of the Thirty-Second Association for the Advancement of Artificial Intelligence (AAAI-18)}. Also accepted for oral presentation and awarded Google travel grant and AAAI grant.
%
%\item \textbf{Thresholding Bandits with Augmented UCB:} In this work, we focus on a variant of the stochastic bandit model called the Thresholding Bandit Problem. We propose the Augmented-UCB (AugUCB) algorithm for a fixed-budget version of the thresholding bandit problem (TBP), where the objective is to identify a set of arms whose quality is above a threshold. A key feature of AugUCB is that it uses both mean and variance estimates to eliminate arms that have been sufficiently explored; to the best of our knowledge, this is the first algorithm to employ such an approach for the considered TBP.  Theoretically, we obtain an upper bound on the loss (probability of misclassification) incurred by AugUCB. Although UCBEV in literature provides a better guarantee, it is important to emphasize that UCBEV has access to problem complexity (whose computation requires arms' mean and variances), and hence is not realistic in practice; this is in contrast to AugUCB whose implementation does not require any such complexity inputs. We conduct extensive simulation experiments to validate the performance of AugUCB. Through our simulation work, we establish that AugUCB, owing to its utilization of variance estimates, performs significantly better than the state-of-the-art APT, CSAR and other non variance-based algorithms. This work has been published in \textit{Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)}.
%\end{enumerate}

%    Another important collaboration was with \textit{Dr. Odalric Maillard} during a three-month internship at INRIA, SequeL Lab, Lille, France from September, 2017 to November, 2017. We worked in the area of piecewise stochastic MAB where there are changepoints when the distribution associated with each arm/action changes abruptly. We devised actively adaptive algorithms to work in this environment, and we are soon planning to publish our research output. This internship has been an eye-opening experience for me where I worked with researchers explicitly on open problems and it was very exciting to get accustomed to the environment of a full-fledged research lab. 
%    
%    Further, I am going for a three-month internship in Adobe Research, San Jose, USA from January 2018 - April 2018 under the supervision of \textit{Dr. Branislav Kveton} to work in the area of conservative contextual bandits. I have planned this internship so that I can actively engage with industry researchers and gain a full-some experience on how research is conducted in industrial labs and how it affects the day-to-day life of users interacting with industrial products. 

%%%%%%%%%%%%%% CMU 
%    Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research; there are several professors at Carnegie Mellon University such as \textit{Dr. Pradeep Ravikumar} and \textit{Dr. Barnab치s P칩czos} whose projects are especially appealing to me. Dr. Pradeep Ravikumar works in theoretical machine learning and its applications to high dimensional real-life data-sets which is an area I am interested to work in. Specifically, I want to work with him in the theoretical areas of learning theory and find common solutions where bandit theory can be applied. Also, I am interested to work with Dr. Barnab치s P칩czos in the area of online learning and optimization. I want to collaborate with him in several of his areas of interest such as automated discovery and efficient data processing in applied sciences including health-sciences and robotics, where there are several bandit variants which can be applied.

%%%%%%%%%%%%%% Stanford
%	Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research; there are several professors at Stanford such as \textit{Dr. Emma Brunskill} and \textit{Dr. Stefano Ermon} whose projects are especially appealing to me. Dr. Emma Brunskill works in theoretical reinforcement  learning and its applications to real-life data-sets which is an area I am interested to work in. Specifically, I want to work with her in the theoretical areas of learning theory and find common solutions where bandit theory can be applied. Also, I am interested to work with Dr. Stefano Ermon in the larger area of artificial intelligence. I want to collaborate with him in several of his areas of interest such as large-scale combinatorial optimization, and robust decision making under uncertainty, where there are several bandit variants which can be applied.

    
%%%%%%%%%%%%%% UMASS
%	Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research, there are several professors at UMASS University whose projects are especially appealing to me such as Dr. Akshay Krishnamurthy and Dr. Philip S. Thomas.
% (Akshay My research interests are in machine learning and statistics. I am specifically interested in interactive learning and learning settings involving feedback-driven data collection. My thesis focuses on interactive approaches for discovering and exploiting low-dimensional structure in data. More recently, I have been studying complex decision making problems with limited feedback, which fall under the umbrella of reinforcement learning.


%	Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research; there are several professors at UMASS such as \textit{Dr. Akshay Krishnamurthy} and \textit{Dr. Philip S. Thomas} whose projects are especially appealing to me. Dr. Akshay Krishnamurthy works in machine learning, statistics,  reinforcement learning and its applications to real-life data-sets which is an area I am interested to work in. Specifically, I want to work with him  in the theoretical areas of learning theory and find common solutions where bandit theory can be applied. Also, I am interested to work with Dr. Philip S. Thomas in the larger area of artificial intelligence and Reinforcement learning. I want to collaborate with him in several of his areas of interest such as options in hierarchical Reinforcement learning, and decision making under uncertainty, where there are several bandit variants which can be applied.

%%%%%%%%%%%%%%%%%%
%Princeton
%	Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research; there are several professors at Princeton such as \textit{Dr. Elad Hazan} and \textit{Dr. Sanjeev Arora} whose projects are especially appealing to me. Dr. Elad Hazan works in machine learning, statistics, and online convex optimization which is an area I am interested to work in. Specifically, I want to work with him  in the theoretical areas of learning theory and find common solutions where bandit theory can be applied. Also, I am interested to work with Dr. Sanjeev Arora in the of theoretical Machine Learning. I want to collaborate with him in several of his areas of interest such as non-convex optimization, un-supervised learning and finding theoretical guarantees of algorithms applied in these areas.


%%%%%%%%%%%%%%%%%%%%%
% Caltech
%%%%%%%%%%%%%%%%%%%%%
%	Although I have mainly focused on immediate Reinforcement Learning settings and mostly theoretical works, I am open to a variety of research; there are several professors at California Institute of Technology such as \textit{Dr. Yisong Yue} and \textit{Dr. Joel A. Tropp} whose projects are especially appealing to me. Dr. Yisong Yue works in theoretical machine learning, recommender systems and its applications to real-life data-sets which is an area I am interested to work in. Specifically, I want to work with him in the theoretical areas of learning theory and find common solutions where bandit theory can be applied. Also, I am interested to work with Dr. Joel A. Tropp in the area of machine learning and optimization problems. I want to collaborate with him in several of his areas of interest such as sparse approximations in machine learning, randomized algorithms and efficient sampling techniques.

%\newpage
\bibliographystyle{aaai}
\bibliography{refs}

%mukherjee2016
%mukherjee2018

\end{document}

